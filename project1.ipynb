{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddeb2504",
   "metadata": {},
   "source": [
    "## Exploring the content of a text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "298e6acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 10000\n"
     ]
    }
   ],
   "source": [
    "# Q1\n",
    "# load the data file\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "with open('data/yelp.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# count rows in this dataset\n",
    "print('Number of reviews:', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "572db2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique user ids: 6403\n"
     ]
    }
   ],
   "source": [
    "# Q2\n",
    "# count number of unique user ids in this dataset\n",
    "user_ids = [entry['user_id'] for entry in data]\n",
    "unique_user_ids = set(user_ids)\n",
    "\n",
    "print('Number of unique user ids:', len(unique_user_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c058eef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average review length: 152.062 words\n",
      "Shortest review length: 1 words\n",
      "Longest review length: 1137 words\n",
      "Content of the shortest review: ['Excellent', 'X', 'Go']\n"
     ]
    }
   ],
   "source": [
    "# Q3\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "reviews = [entry['text'] for entry in data]\n",
    "\n",
    "# tokenize each review into words and calculate the length of each review in words\n",
    "review_lengths = [len(word_tokenize(review)) for review in reviews]\n",
    "\n",
    "# calculate the average words of reviews \n",
    "average_words = sum(review_lengths) / len(reviews)\n",
    "\n",
    "# find the shortest and longest reviews length\n",
    "shortest_review_words = min(review_lengths)\n",
    "longest_review_words = max(review_lengths)\n",
    "\n",
    "# get the content of the shortest review\n",
    "shortest_review_content = []\n",
    "for entry in data:\n",
    "    if len(word_tokenize(entry[\"text\"])) == shortest_review_words:\n",
    "        shortest_review_content.append(entry[\"text\"])\n",
    "\n",
    "# print the results\n",
    "print('Average review length:', average_words, 'words')\n",
    "print('Shortest review length:', shortest_review_words, 'words')\n",
    "print('Longest review length:', longest_review_words, 'words')\n",
    "print('Content of the shortest review:', shortest_review_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7f331dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average review length: 9.2126 sentences\n",
      "Shortest review length: 1 sentences\n",
      "Longest review length: 92 sentences\n",
      "The longest review by sentences is not the same as the longest review by words.\n"
     ]
    }
   ],
   "source": [
    "# Q4\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "reviews = [entry['text'] for entry in data]\n",
    "\n",
    "# tokenize each review into sentences and calculate the number of sentences in each review\n",
    "review_sentence_lengths = [len(sent_tokenize(review)) for review in reviews]\n",
    "\n",
    "# calculate the average length of reviews in sentences\n",
    "average_sentence_length = sum(review_sentence_lengths) / len(reviews)\n",
    "\n",
    "# find the shortest and longest reviews in sentences\n",
    "shortest_review_sentences = min(review_sentence_lengths)\n",
    "longest_review_sentences = max(review_sentence_lengths)\n",
    "\n",
    "# get the longest reviews content in sentences\n",
    "longest_sentence_review_index = review_sentence_lengths.index(longest_review_sentences)\n",
    "longest_sentence_review_content = reviews[longest_sentence_review_index]\n",
    "\n",
    "# get the longest reviews content in words\n",
    "longest_review_index = review_lengths.index(longest_review_words)\n",
    "longest_review_content = reviews[longest_review_index]\n",
    "\n",
    "# print the results\n",
    "print('Average review length:', average_sentence_length, 'sentences')\n",
    "print('Shortest review length:', shortest_review_sentences, 'sentences')\n",
    "print('Longest review length:', longest_review_sentences, 'sentences')\n",
    "\n",
    "# Check if the longest review by sentences is the same as the longest review by words\n",
    "if longest_sentence_review_content == longest_review_content:\n",
    "    print(\"The longest review by sentences is the same as the longest review by words.\")\n",
    "else:\n",
    "    print(\"The longest review by sentences is not the same as the longest review by words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca6581ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ten most common tokens and counts:\n",
      "[('.', 76320),\n",
      " ('the', 55130),\n",
      " (',', 54520),\n",
      " ('and', 42581),\n",
      " ('I', 40331),\n",
      " ('a', 35110),\n",
      " ('to', 29935),\n",
      " ('was', 20753),\n",
      " ('of', 20729),\n",
      " ('is', 17623)]\n",
      "\n",
      "Ten least common tokens and counts:\n",
      "[('filled/stuffed', 1),\n",
      " ('waznt', 1),\n",
      " ('pretensiousness', 1),\n",
      " ('seemlessly', 1),\n",
      " ('prunes', 1),\n",
      " ('obtainable', 1),\n",
      " ('5minutes', 1),\n",
      " ('WORK', 1),\n",
      " ('Spinatos', 1),\n",
      " ('altering', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Q5\n",
    "\n",
    "# count tokens in review\n",
    "from collections import Counter\n",
    "\n",
    "reviews = [entry['text'] for entry in data]\n",
    "\n",
    "# create an empty Counter\n",
    "token_counts = Counter()\n",
    "\n",
    "# tokenize each review and update the Counter\n",
    "for review in reviews:\n",
    "    tokens = word_tokenize(review)\n",
    "    token_counts.update(tokens)\n",
    "\n",
    "# Ten most common tokens\n",
    "print('Ten most common tokens and counts:')\n",
    "pprint(token_counts.most_common(10))\n",
    "print()\n",
    "\n",
    "# Ten least common tokens\n",
    "print('Ten least common tokens and counts:')\n",
    "pprint(token_counts.most_common()[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a94c108",
   "metadata": {},
   "source": [
    "Counting word tokens without further processing can lead to inflated token tounts, specifically, different word forms are counted separately and cause an overestimation of vocabulary diversity.I could also create difficulty in matching. Text retrieval and matching tasks may be less effective without normalizing the words. In addition,typos and misspellings can introduce noise in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51367d87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ten most common tokens and counts:\n",
      "[('place', 7474),\n",
      " ('good', 6921),\n",
      " ('food', 6285),\n",
      " ('like', 5626),\n",
      " ('great', 5115),\n",
      " ('go', 4882),\n",
      " ('get', 4656),\n",
      " ('time', 4603),\n",
      " ('one', 4290),\n",
      " ('order', 3659)]\n",
      "\n",
      "Ten least common tokens and counts:\n",
      "[('602', 1),\n",
      " ('en', 1),\n",
      " ('augment', 1),\n",
      " ('crisscross', 1),\n",
      " ('dugout', 1),\n",
      " ('unintend', 1),\n",
      " ('waznt', 1),\n",
      " ('pretensi', 1),\n",
      " ('seemlessli', 1),\n",
      " ('5minut', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Q6\n",
    "# stem tokens in review\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "# reference: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "reviews = [entry['text'] for entry in data]\n",
    "\n",
    "# create an empty Counter that collect stemmed tokens\n",
    "stem_token_counts = Counter()\n",
    "\n",
    "# tokenize each review and stem each token\n",
    "for review in reviews:\n",
    "    # tokenize the review and convert to lowercase\n",
    "    tokens = word_tokenize(review.lower())\n",
    "    # remove punctuation, remove stop words and stem the tokens\n",
    "    stemmed_tokens = [ps.stem(t) for t in tokens if t not in stop_words and t.isalnum()]\n",
    "    stem_token_counts.update(stemmed_tokens)\n",
    "\n",
    "# Ten most common tokens\n",
    "print('Ten most common tokens and counts:')\n",
    "pprint(stem_token_counts.most_common(10))\n",
    "print()\n",
    "\n",
    "# Ten least common tokens\n",
    "print('Ten least common tokens and counts:')\n",
    "pprint(stem_token_counts.most_common()[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaa0172",
   "metadata": {},
   "source": [
    "Compare to problem 5, the preprocessing steps of question 6 cleaned up the text data and removed noise, leading to more meaningful and representative results in terms of common and rare tokens.For example, the most common tokens in question 6 include words like \"place,\" \"good,\" \"food,\" \"like,\" \"great,\" and \"go.\" These are likely to be more semantically meaningful words in the context of restaurant reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1532dff9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most prevalent unigrams:\n",
      "[('the', 66739)]\n",
      "\n",
      "Most prevalent bigrams:\n",
      "[(('of', 'the'), 4488)]\n",
      "\n",
      "Most prevalent trigrams:\n",
      "[(('this', 'place', 'is'), 708)]\n"
     ]
    }
   ],
   "source": [
    "# Q7\n",
    "from nltk.util import ngrams\n",
    "\n",
    "unigram_counts = Counter()\n",
    "bigram_counts = Counter()\n",
    "trigram_counts = Counter()\n",
    "\n",
    "for entry in data:\n",
    "    review_text = entry['text']\n",
    "    tokens = word_tokenize(review_text.lower())\n",
    "    tokens = [t for t in tokens if t.isalnum()]\n",
    "\n",
    "    unigrams = tokens\n",
    "    bigrams = list(ngrams(tokens, 2))\n",
    "    trigrams = list(ngrams(tokens, 3))\n",
    "\n",
    "    unigram_counts.update(unigrams)\n",
    "    bigram_counts.update(bigrams)\n",
    "    trigram_counts.update(trigrams)\n",
    "\n",
    "print('Most prevalent unigrams:')\n",
    "pprint(unigram_counts.most_common(1))\n",
    "print()\n",
    "print('Most prevalent bigrams:')\n",
    "pprint(bigram_counts.most_common(1))\n",
    "print()\n",
    "print('Most prevalent trigrams:')\n",
    "pprint(trigram_counts.most_common(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bc6fb0",
   "metadata": {},
   "source": [
    "'the' and 'of the'are all common stop word. Though 'this place is' can't be considered as stop word, it is a common phrase used in restaurant reviews and may not carry much specific meaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2e7fc5d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most prevalent unigrams (after stop word removal):\n",
      "[('good', 6762)]\n",
      "\n",
      "Most prevalent bigrams (after stop word removal):\n",
      "[(('happy', 'hour'), 600)]\n",
      "\n",
      "Most prevalent trigrams (after stop word removal):\n",
      "[(('sweet', 'potato', 'fries'), 99)]\n"
     ]
    }
   ],
   "source": [
    "# Q7 Bonus\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "unigram_counts = Counter()\n",
    "bigram_counts = Counter()\n",
    "trigram_counts = Counter()\n",
    "\n",
    "for entry in data:\n",
    "    review_text = entry['text']\n",
    "    tokens = word_tokenize(review_text.lower())\n",
    "    tokens = [t for t in tokens if t.isalnum() and t not in stop_words]\n",
    "\n",
    "    unigrams = tokens\n",
    "    bigrams = list(ngrams(tokens, 2))\n",
    "    trigrams = list(ngrams(tokens, 3))\n",
    "    \n",
    "    unigram_counts.update(unigrams)\n",
    "    bigram_counts.update(bigrams)\n",
    "    trigram_counts.update(trigrams)\n",
    "\n",
    "print('Most prevalent unigrams (after stop word removal):')\n",
    "pprint(unigram_counts.most_common(1))\n",
    "print()\n",
    "print('Most prevalent bigrams (after stop word removal):')\n",
    "pprint(bigram_counts.most_common(1))\n",
    "print()\n",
    "print('Most prevalent trigrams (after stop word removal):')\n",
    "pprint(trigram_counts.most_common(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7263aa5b",
   "metadata": {},
   "source": [
    "Yes, they are generally more meaningful after removing stop words, these words are more informative and could help identify positive or negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "342d7721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-3 most similar reviews:\n",
      "I have been going here for years. I love the \"Hong Kong\" style pan fried crispy noodles!\n",
      "Shaved noodles, shaved noodles, shaved noodles!  Am I too butt lazy if I only go for shaved noodles?  The dish ordered with the handpulled meant I had to hold chopsticks high into the air trying to separate a portion of noodles out onto our plates.  Not bad but wasn't special... but the shaved noodles, loved the chewy texture.  I unashamedly used my baby godson as an excuse to get a mini noodle pulling show at the window.\n",
      "\n",
      "I'm going to work my way thru the fresh fruit juice offerings...  I used to drink that daily while working as an expat in HK.  Speaking of which, the bathrooms are sooooo HK grand (Amy L, you and I think alike!)...\n",
      "\n",
      "Overall the stir fried dishes were a bit greasy and the flavors decent, but the draw here is the shaved noodles (can I plug this anymore?!) and the fresh fruit juice.\n",
      "I've been going here for the past 9+ years and it's pretty much the basic local circle-K. Not much to say past that. I do dig the walk-in beer cooler but also wonder who in the hell buys the bags of bananas?\n"
     ]
    }
   ],
   "source": [
    "# Q8\n",
    "# reference: https://saturncloud.io/blog/how-to-calculate-tfidf-using-sklearn-for-ngrams-in-python/\n",
    "# reference: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "# reference: https://stackoverflow.com/questions/47898326/how-vectorizer-fit-transform-work-in-sklearn\n",
    "# reference: https://www.geeksforgeeks.org/numpy-argsort-in-python/\n",
    "\n",
    "with open('data/query.txt', 'r') as f:\n",
    "    query = f.read()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "reviews = [entry[\"text\"] for entry in data]\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3),min_df=10)\n",
    "reviews_tfidf = vectorizer.fit_transform(reviews)\n",
    "query_tfidf = vectorizer.transform([query])\n",
    "\n",
    "cosine_sim = cosine_similarity(query_tfidf, reviews_tfidf)\n",
    "similarity_scores = cosine_sim[0]\n",
    "top_3_indices = np.argsort(similarity_scores)[-3:][::-1]\n",
    "\n",
    "print(\"Top-3 most similar reviews:\")\n",
    "for idx in top_3_indices:\n",
    "    print(data[idx]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4264fc62",
   "metadata": {},
   "source": [
    "Strengths:\n",
    "Easy to implement and understand.\n",
    "Captures n-gram significance, considering both individual words and phrases.\n",
    "Feature extraction through TF-IDF allows for informative comparisons.\n",
    "\n",
    "Weaknesses:\n",
    "Overemphasis on frequency.\n",
    "Does not consider context or relationships between n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777189af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
